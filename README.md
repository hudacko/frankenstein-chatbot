A Large Language Model (LLM) is a deep learning algorithm “trained” on massive datasets of text to recognize patterns
and predict the most likely next word in a sequence much like an autocomplete function.  After training, the algorithm
can understand and generate human-like language.  This is  what is commonly called a “chatbot”.  The LLM for this paper
will be bounded to biomedicalization topics, Mary Shelley’s Frankenstein and Ridley Scott’s two Alien prequels, Prometheus
and Alien: Covenant.  This bot aims to make the study of biomedicalization theory understandable through familiar narratives.
